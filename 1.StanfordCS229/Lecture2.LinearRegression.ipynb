{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 \n",
    "https://youtu.be/5u4G23_OohI\n",
    "\n",
    "## Linear regression \n",
    "Running example: housing problem in Portland, OR  \n",
    "![housing](img/L2F1.png)\n",
    "\n",
    "\n",
    "Plot this data set:\n",
    "![housing_plot](img/L2F2.png)\n",
    "\n",
    "\n",
    "Notation:\n",
    "- m = training examples\n",
    "- x = \"input\" variables / features \n",
    "- y = \"ouput\" variable / \"target\" vriable \n",
    "- (x, y) --> training example \n",
    "- $j^{th}$ traing example = ($x^{(j)},y^{(j)}$)\n",
    "\n",
    "![learning](img/L2F3.png)\n",
    "\n",
    "h = hypothesis  \n",
    "e.g., $h(x) = \\theta_0 + \\theta_1x$  \n",
    "x: input feature -> size of the house \n",
    "\n",
    "If more than one features, e.g., number of bedrooms of the houses  \n",
    "$x_1$: size  \n",
    "$x_2$: number of bedroom  \n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x_1+\\theta_2x_2$  \n",
    "\n",
    "For conciseness, define $x_0=1$, $h_\\theta(x) = \\sum_{i=0}^{2} \\theta_ix_i = \\theta^Tx$    \n",
    "  \n",
    "If n = number of features, $h_\\theta(x) = \\sum_{j=0}^{n} \\theta_jx_j = \\theta^Tx$    \n",
    "  \n",
    "$\\theta_j$: parameters:\n",
    "- parameters of learning algorithms  \n",
    "- real numbers  \n",
    "- a job of learnng algorithm to use the training set to choose / learn appropriate parameters  \n",
    "\n",
    "**Learning Problem**: how do we choose the parameters $\\theta$ so that hypothesis $H$ will make accurate predictions about $y$  \n",
    "- use the preciction of learning algorithm abstract the training set ($m$ training examples)  \n",
    "- $\\min_\\theta \\frac{1}{2} \\sum_{j=1}^{m} (h_\\theta(x^{(j)})-y^{(j)})^2$\n",
    "- $\\frac{1}{2}$: simplify the math  \n",
    "  \n",
    "$\\to$\n",
    "Least square regression:\n",
    "- $J(\\theta) = \\frac{1}{2} \\sum_{j=1}^{m} (h_\\theta(x^{(j)})-y^{(j)})^2$\n",
    "- $\\min_\\theta J(\\theta)$\n",
    "\n",
    "***\n",
    "***\n",
    "## Gradient descent\n",
    "- Search algorithm \n",
    "- Start with some $\\theta$, set  $\\theta = \\bar{0}$ --> vector of all 0  \n",
    "- Keep changing $\\theta$ to reduce $J(\\theta)$, until end up at $\\min_\\theta J(\\theta)$   \n",
    "\n",
    "Video:[Gradient Descent](https://youtu.be/F6GSRDoB-Cg?t=135)  \n",
    "\n",
    "- Look around \n",
    "- what is the direction that would take you downhill as quickly as possible  \n",
    "- End up at a local minimum  \n",
    "- **With a slightly different initial local starting point, can end up at a completely different locla optimal**  \n",
    "![gradient descent](img/L2F4.png)  \n",
    "\n",
    "\n",
    "Definition:\n",
    "![gradient descent](img/L2F5.png) \n",
    "- The update is simultaneously performed for all values of $J=0,...,n$  \n",
    "- $\\alpha$: learning rate  \n",
    "- $:=$ means right hand side ovride the left hand side  \n",
    "  \n",
    "  \n",
    "- if only have training example $(x, y)$  \n",
    "![gradient descent](img/L2F6.png) \n",
    "- for a single training example, the **update rule**:  \n",
    "![update rule](img/L2F7.png) \n",
    "\n",
    "#### **Batch Gradint Descent**: for every $j$:  \n",
    "\n",
    "![update rule](img/L2F8.png)  \n",
    "\n",
    "- linear regression has **only one global**, and no other local, optima  \n",
    "- gradient descent always converges   \n",
    "\n",
    "![update rule](img/L2F9.png)  \n",
    "\n",
    "\n",
    "#### **Stochastic Gradient Descent (Incremental Gradient Descent)**  \n",
    "- efficient when m (the number of training exmples) is large  \n",
    "-  each time encounter one training example, update the parameters according to the gradient of the error with respect to that single training example only  \n",
    "- won't acturally converge to the global optimal; in practice, most of the values near the minimum will be reasonably good approximations to the true minimum  \n",
    "![update rule](img/L2F10.png)  \n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "## Normal equations \n",
    "\n",
    "Matrix-vectorial notation:  \n",
    "- Design matrix $X$:  \n",
    "![update rule](img/L2F11.png)  \n",
    "- Target value $\\bar{y}$:  \n",
    "![update rule](img/L2F12.png)  \n",
    "\n",
    "\n",
    "\n",
    "![update rule](img/L2F13.png)  \n",
    "![update rule](img/L2F14.png)  \n",
    "- derivatives with respect to $\\theta$\n",
    "https://youtu.be/5u4G23_OohI?t=4355\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
